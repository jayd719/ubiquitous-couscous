{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b17908-6187-4f55-8cd9-591293a2fc0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'real.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 133\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(conf_matrix)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 133\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m fake_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update path if necessary\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m preprocess_data(df)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(real_path, fake_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m(real_path, fake_path):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m         real_headlines \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fake_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'real.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def load_data(real_path, fake_path):\n",
    "    with open(real_path, 'r', encoding='utf-8') as file:\n",
    "        real_headlines = file.readlines()\n",
    "    with open(fake_path, 'r', encoding='utf-8') as file:\n",
    "        fake_headlines = file.readlines()\n",
    "    \n",
    "    # Preprocess: Remove extra spaces and newlines\n",
    "    real_headlines = [line.strip().lower() for line in real_headlines if line.strip()]\n",
    "    fake_headlines = [line.strip().lower() for line in fake_headlines if line.strip()]\n",
    "    \n",
    "    # Create DataFrame with labels\n",
    "    df_real = pd.DataFrame({'headline': real_headlines, 'label': 1})  # Real = 1\n",
    "    df_fake = pd.DataFrame({'headline': fake_headlines, 'label': 0})  # Fake = 0\n",
    "    \n",
    "    # Combine datasets\n",
    "    df = pd.concat([df_real, df_fake], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Split dataset into train (70%) and test (30%)\n",
    "    return train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "\n",
    "def extract_features(train_data, test_data):\n",
    "    # Convert text into numerical feature vectors\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X_train_counts = vectorizer.fit_transform(train_data['headline'])\n",
    "    X_test_counts = vectorizer.transform(test_data['headline'])\n",
    "    return vectorizer, X_train_counts, X_test_counts\n",
    "\n",
    "def filter_words(vectorizer, X_train_counts):\n",
    "    # Compute document frequency\n",
    "    doc_freq = np.asarray((X_train_counts > 0).sum(axis=0)).flatten()\n",
    "    \n",
    "    # Define thresholds (words in >70% or <0.5% of headlines are removed)\n",
    "    min_threshold = 0.005 * X_train_counts.shape[0]\n",
    "    max_threshold = 0.70 * X_train_counts.shape[0]\n",
    "    \n",
    "    # Select valid words\n",
    "    valid_indices = np.where((doc_freq >= min_threshold) & (doc_freq <= max_threshold))[0]\n",
    "    \n",
    "    # Filter vocabulary\n",
    "    valid_vocab = [vectorizer.get_feature_names_out()[i] for i in valid_indices]\n",
    "    filtered_vectorizer = CountVectorizer(stop_words='english', vocabulary=valid_vocab)\n",
    "    \n",
    "    return filtered_vectorizer\n",
    "\n",
    "def train_naive_bayes(X_train_filtered, train_data):\n",
    "    alpha = 1  # Laplace smoothing factor\n",
    "    \n",
    "    # Compute priors\n",
    "    total_real = train_data['label'].sum()\n",
    "    total_fake = len(train_data) - total_real\n",
    "    P_real = total_real / len(train_data)\n",
    "    P_fake = total_fake / len(train_data)\n",
    "    \n",
    "    # Convert labels to NumPy array for proper indexing\n",
    "    real_indices = np.array(train_data['label'] == 1)\n",
    "    fake_indices = np.array(train_data['label'] == 0)\n",
    "    \n",
    "    # Compute word counts\n",
    "    X_train_real = X_train_filtered[real_indices]\n",
    "    X_train_fake = X_train_filtered[fake_indices]\n",
    "    \n",
    "    word_count_real = np.asarray(X_train_real.sum(axis=0)).flatten() + alpha\n",
    "    word_count_fake = np.asarray(X_train_fake.sum(axis=0)).flatten() + alpha\n",
    "    \n",
    "    # Compute conditional probabilities\n",
    "    P_word_given_real = word_count_real / word_count_real.sum()\n",
    "    P_word_given_fake = word_count_fake / word_count_fake.sum()\n",
    "    \n",
    "    # Convert to log probabilities for numerical stability\n",
    "    log_P_real = np.log(P_real)\n",
    "    log_P_fake = np.log(P_fake)\n",
    "    log_P_word_given_real = np.log(P_word_given_real)\n",
    "    log_P_word_given_fake = np.log(P_word_given_fake)\n",
    "    \n",
    "    return log_P_real, log_P_fake, log_P_word_given_real, log_P_word_given_fake\n",
    "\n",
    "\n",
    "def predict_naive_bayes(X_test_filtered, log_P_real, log_P_fake, log_P_word_given_real, log_P_word_given_fake):\n",
    "    # Convert test data to dense array for matrix multiplication\n",
    "    X_test_counts = X_test_filtered.toarray()\n",
    "    \n",
    "    # Compute posterior probabilities for each class\n",
    "    log_P_real_test = log_P_real + X_test_counts @ log_P_word_given_real.T\n",
    "    log_P_fake_test = log_P_fake + X_test_counts @ log_P_word_given_fake.T\n",
    "    \n",
    "    # Predict class (1 if log P(Real) > log P(Fake), else 0)\n",
    "    return (log_P_real_test > log_P_fake_test).astype(int)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    return accuracy, conf_matrix\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    real_path = \"real.txt\"  # Update path if necessary\n",
    "    fake_path = \"fake.txt\"  # Update path if necessary\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_data(real_path, fake_path)\n",
    "    train_data, test_data = preprocess_data(df)\n",
    "    \n",
    "    # Extract features\n",
    "    vectorizer, X_train_counts, X_test_counts = extract_features(train_data, test_data)\n",
    "    \n",
    "    # Filter out uninformative words\n",
    "    filtered_vectorizer = filter_words(vectorizer, X_train_counts)\n",
    "    X_train_filtered = filtered_vectorizer.fit_transform(train_data['headline'])\n",
    "    X_test_filtered = filtered_vectorizer.transform(test_data['headline'])\n",
    "    \n",
    "    # Train Na√Øve Bayes model\n",
    "    log_P_real, log_P_fake, log_P_word_given_real, log_P_word_given_fake = train_naive_bayes(X_train_filtered, train_data)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = predict_naive_bayes(X_test_filtered, log_P_real, log_P_fake, log_P_word_given_real, log_P_word_given_fake)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy, conf_matrix = evaluate_model(test_data['label'].to_numpy(), y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29214fe5-6723-4b3c-ba3c-b5a6081e33c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
